# Specify server locations in a SOLR_LOCATOR variable;
# used later in variable substitutions
# Change the zkHost to point to your own Zookeeper quorum
SOLR_LOCATOR : {
  # Name of solr collection
  collection :enronemail

  # ZooKeeper ensemble
  zkHost : "$ZK_HOST"
}

# Specify an array of one or more morphlines, each of which defines an ETL
# transformation chain. A morphline consists of one or more (potentially
# nested) commands. A morphline is a way to consume records (e.g. Flume events,
# HDFS files or blocks), turn them into a stream of records, and pipe the stream
# of records through a set of easily configurable transformations on it's way to
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
{
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a
    # morphline config file
    id : morphline1
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "com.cloudera.**", "org.apache.solr.**"]
    commands : [
    {
        ## Read the email stream and break it up into individual messages.
        ## The beginning of a message is marked by regex clause below
        ## The reason we use this command is that one event can have multiple
        ## messages
        readMultiLine {
            regex : "Message-ID:.*"
            what : next
            charset : UTF-8
        }
    }
    ## Break up the email text into SOLR fields
    {
        if {
            conditions: [
            {
                not{
                    grok {
                        expressions : {
                            message: """(?s)(.*?)(Message-ID: <)(?<message_id>(.+?))(>.*?)(Date: )(?<date>(.+?))( \(.+?)(From: )(?<from>(.*?))((To: )(?<to>(.+?)))?(Subject: )(?<subject>(.*?))((Cc: )(?<cc>(.*)))?(Mime.+?)((Bcc: )(?<bcc>(.*)))?(X-From: )(?<from_names>(.*?))(X-To: )(?<to_names>(.*?))(X-cc: )(?<cc_names>(.*?))(X-bcc: )(?<bcc_names>(.*?))(X-Folder: )(?<x_folder>(.*?))(X-Origin: )(?<x_origin>(.*?))(X-FileName: )(?<x_filename>(.*?))(\n)(?<body>(.*))"""
                        }
                        extract: inplace
                        findSubstrings: false
                        addEmptyStrings: false
                        numRequiredMatches: all
                    }
                }
            }
            ]
            then:[
            { logInfo { format : "found no grok match: {}", args : ["@{}"] } }
            { dropRecord {} }
            ]
        }
    }

    # add Unique ID, in case our message_id field from above is not present
    {
        generateUUID {
            field:id
        }
    }

    # convert the timestamp field to "yyyy-MM-dd'T'HH:mm:ss.SSSZ" format
    {
        convertTimestamp {
            field : date
            inputFormats : ["EEE, d MMM yyyy HH:mm:ss Z", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd'T'HH:mm:ss", "yyyy-MM-dd"]
            inputTimezone : America/Los_Angeles
            outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
            outputTimezone : UTC
        }
    }

    # Consume the output record of the previous command and pipe another
    # record downstream.
    #
    # This command sanitizes record fields that are unknown to Solr schema.xml
    # by deleting them. Recall that Solr throws an exception on any attempt to
    # load a document that contains a field that isn't specified in schema.xml
    {
        sanitizeUnknownSolrFields {
            # Location from which to fetch Solr schema
            solrLocator : ${SOLR_LOCATOR}
        }
    }

    # load the record into a SolrServer or MapReduce SolrOutputFormat.
    {
        loadSolr {
            solrLocator : ${SOLR_LOCATOR}
        }
    }
    ]
}
]
